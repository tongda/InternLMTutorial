# 课程及InternLM2 技术报告笔记

## 书生·浦语大模型全链路开源体系

总结如下：

1. **通用人工智能的进展方向**：从专注于单一任务的模型转向通用大模型，能够应对多种任务和模态。

2. **书生·浦语模型升级**：在7月进行了升级，支持8K上下文和工具体系；在8月发布了对话模型和智能体框架；并在9月推出了中等尺寸模型与优化工具链；2024 年 1 月 17 日，InternLM2开源。

3. **InternLM2的开源**：提升了模型性能，以应对复杂场景，并提供了不同尺寸（7B 和 20B，相比国内其他厂商只开源 7B，InternLM2 非常有诚意）的模型以满足不同需求。

4. **模型的核心能力**：包括长上下文理解、对话与创作、数学能力等，例如进行行程规划和情感对话等任务。

5. **InternLM2的优化**：通过数据清洗、高质量语料和新数据补全来提升模型性能（Loss 分布左移）。

6. **下游任务的性能提升**：模型使用更少数据也能达到上一代效果，整体性能得到了提升。

7. **开源工具生态**：包括数据到预训练、微调、部署和评测等全流程工具，如书生万卷数据集，InternLM-Train，XTuner，LMDeploy，OpenCompass，LAgent&AgentLego等。

8. **数据集**：提供了丰富多样的数据，支持数据清洗、安全处理和公开使用。

9. **性能评估与差距**：大模型整体能力仍有提升空间，尤其在理科能力上，在中文场景下国内模型表现出色。

10. **部署解决方案**：LMDeploy支持模型轻量化、量化和推理服务，并与评测工具无缝对接。
